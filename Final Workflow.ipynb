{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b38113ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "948b2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    if \"stop\" in sentences[:-1]:\n",
    "        sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3046a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14716558",
   "metadata": {},
   "source": [
    "### Construct lookup table of \n",
    "* document_uuid to emotion\n",
    "* Document_uuid to list of sentences\n",
    "* Document_uuid tot list of sentence embeddings\n",
    "* Document_uuid to whole and average embed\n",
    "* sentenceblock_uuid to list of corresponding sentences\n",
    "* sentence_block to whole and average embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbcf845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters:\n",
    "window_size = 4\n",
    "connection_threshold = 1.1 # we should construct some form of metric more rigorously here...\n",
    "relaxation_threshold = 1.1\n",
    "num_docs_to_consider_in_score = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "211f7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3):\n",
    "    if n > len(a):\n",
    "        return np.mean(a, axis=0)\n",
    "        \n",
    "    ret = np.cumsum(a, dtype=float, axis=0)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b976e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_window_embedding(sentence_list, n=3):\n",
    "    if n >= len(sentence_list):\n",
    "        return model.encode(\" \".join(sentence_list))\n",
    "    \n",
    "    embeddings = []\n",
    "    for i in range(len(sentence_list)//n + 1):\n",
    "        sentence_block = \" \".join(sentence_list[i:i+n])\n",
    "        embeddings.append(model.encode(sentence_block))\n",
    "        \n",
    "    return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dd7df076",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_emotion = {} # docid: emotion\n",
    "doc_to_source = {} # docid: source\n",
    "doc_to_wordcounts = {} # docid: length\n",
    "doc_to_sentences = {} # docid: [list of sentences]\n",
    "doc_to_sentembeddings = {} # docid: {raw:[listoforderedembeddings], rolling_average:[listofslidingavgs], windowed:[listofwindoweds]}\n",
    "doc_to_docembeddings = {} # docid: {'average':embed, 'whole':embed}\n",
    "\n",
    "for directory in [\"./friendship/\", \"./loneliness/\"]:\n",
    "    emotion = directory.split('/')[1]    \n",
    "    for filename in os.listdir(directory):\n",
    "        doc_id = uuid.uuid4()\n",
    "        txtstring = open(directory + filename, 'r').read().split(\"|_|\")\n",
    "        whole_doc_text = txtstring[0].replace(\"\\n\", \" \")\n",
    "        doc_sentences = split_into_sentences(whole_doc_text)\n",
    "        doc_sentences = [s for s in doc_sentences if (s != \".\" and s != '')]\n",
    "        \n",
    "        try:\n",
    "            source = txtstring[1]\n",
    "        except IndexError as e:\n",
    "            source = \"Unknown\"\n",
    "         \n",
    "        sentence_embeddings = np.array(model.encode(doc_sentences))\n",
    "        whole_embedding = model.encode(whole_doc_text)\n",
    "        average_embedding = np.mean(sentence_embeddings, axis=0)\n",
    "        raw_sentence_embeddings = sentence_embeddings\n",
    "        rolling_average_sentence_embeddings = moving_average(raw_sentence_embeddings, window_size)\n",
    "        sentence_block_embeddings = moving_window_embedding(doc_sentences, window_size)\n",
    "        \n",
    "        doc_to_emotion[doc_id] = emotion\n",
    "        doc_to_source[doc_id] = source\n",
    "        doc_to_wordcounts[doc_id] = len(whole_doc_text.split())\n",
    "        doc_to_sentences[doc_id] = doc_sentences\n",
    "        doc_to_docembeddings[doc_id] = {'average':average_embedding, 'whole':whole_embedding}\n",
    "        doc_to_sentembeddings[doc_id] = {\n",
    "            'raw': raw_sentence_embeddings,\n",
    "            'moving_average': rolling_average_sentence_embeddings,\n",
    "            'moving_window': sentence_block_embeddings\n",
    "        }\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d62185b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal = \"\"\"\n",
    "I went with my family yesterday to a beach in Palos Verdes.\n",
    "I enjoyed seeing the way my parents interacted with each other, and love showing my little sister all the plants and stuff and watching her discover them for the first time.\n",
    "I was like, \"do I want kids\"?\n",
    "\"\"\"\n",
    "\n",
    "journal = \"\"\"\n",
    "I worked on my homework quickly so I could join my friends at the party. Every year, my birthday is during dead week, so I never get to celebrate - however, this year, they surprised me with a cake! \n",
    "It was the first good birthday I had in college.\n",
    "\"\"\"\n",
    "journal = \"\"\"\n",
    "I have a friend who lived in Cali for school (I’m east coast.) while she was there, her parents FaceTimed her to tell her they were getting a divorce. She called me, and I immediately booked my $1,000 plane ticket for two days from then. That trip ended up being really fun, but it was so good to be able to be there for her.\n",
    "\n",
    "A few years later, I lost my mom very suddenly. I didn’t know how to cope or move on with life. This same friend came over and spent every night at my apartment listening to me cry and try to figure out how to make funeral arrangements and estate decisions. As much as it sucked, it was awesome having such a good friend, and knowing that either of us would drop everything for each other.\n",
    "\"\"\"\n",
    "\n",
    "journal = \"\"\"\n",
    "Depressed. Relationship of 1.5 years ended a month ago and I'm still depressed about it. I'm just taking it day by day. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "edd639db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the journal entry, we do: wholeembedding? sliding window? or what?\n",
    "whole_journal_embed = model.encode(journal)\n",
    "\n",
    "journal_sentences = [s for s in split_into_sentences(journal) if s != \".\"]\n",
    "journal_sentence_embeddings = np.array(model.encode(journal_sentences))\n",
    "avg_journal_embed = np.mean(journal_sentence_embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "34b8937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53073937\n",
      "0.552917\n",
      "0.58949\n",
      "0.5953817\n",
      "0.5976612\n",
      "-5\n"
     ]
    }
   ],
   "source": [
    "# now, we construct a connection score using nearest neighbors voting, filtering with threshold score.\n",
    "# we construct the scores based on the whole journal embed? the average journal embed? the journal sentence embeddings?\n",
    "# it makes most sense to test wholes and averages\n",
    "# 4 possibilities then: wholel2whle, whole2avg, avg2avg, avg2whole\n",
    "# what if we simply took the construction that was most robust across all 4?\n",
    "\n",
    "w2w = []\n",
    "a2a = []\n",
    "a2w = []\n",
    "w2a = []\n",
    "\n",
    "# doc_ids_to_distance_from_journal = {}\n",
    "doc_ids_distances = []\n",
    "for doc_id, doc_embeddings in doc_to_docembeddings.items():\n",
    "    avg_doc_embed = doc_embeddings['average']\n",
    "    whole_doc_embed = doc_embeddings['whole']\n",
    "    \n",
    "    # we have to have some way of choosing NOT to add a message:\n",
    "    avg2avg = np.linalg.norm(avg_journal_embed - avg_doc_embed)\n",
    "    avg2whole = np.linalg.norm(avg_journal_embed - whole_doc_embed)\n",
    "    whole2whole = np.linalg.norm(whole_journal_embed - whole_doc_embed)\n",
    "    whole2avg = np.linalg.norm(whole_journal_embed - avg_doc_embed)\n",
    "    \n",
    "    mean_distance = sum([avg2avg, avg2whole, whole2whole, whole2avg])/4\n",
    "#     doc_ids_distances.append((doc_id, mean_distance))\n",
    "    \n",
    "    w2w.append((whole2whole, doc_id))\n",
    "    a2a.append((avg2avg, doc_id))\n",
    "    a2w.append((avg2whole, doc_id))\n",
    "    w2a.append((whole2avg, doc_id))\n",
    "    \n",
    "\n",
    "# find the doc that is, on average, closest!\n",
    "sorted_closest_docs = sorted(a2a, key=lambda x: x[0])\n",
    "closest_doc_id = sorted_closest_docs[0][1]\n",
    "\n",
    "# we construct a connection score by looking at, on average, n the closest docs\n",
    "connection_score = 0\n",
    "for (distance, doc_id) in sorted_closest_docs[:num_docs_to_consider_in_score]:\n",
    "    emotion = doc_to_emotion[doc_id]\n",
    "    print(distance)\n",
    "    connection_score += {'friendship':1, 'loneliness':-1}[emotion]\n",
    "\n",
    "print(connection_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b69e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139fffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9ee7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0fe96b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, check for the word length/sentence count of the passage. let's cap it at 110 words? \n",
    "sentences_to_return = doc_to_sentences[closest_doc_id]\n",
    "word_count = doc_to_wordcounts[closest_doc_id]\n",
    "\n",
    "if word_count > 110:\n",
    "    # if the passage is quite long, then identify the closest moving chunks:\n",
    "    moving_window_embeds = doc_to_sentembeddings[closest_doc_id]['moving_window']\n",
    "    moving_avg_embeds = doc_to_sentembeddings[closest_doc_id]['moving_average']\n",
    "    \n",
    "    # find closest \n",
    "    np.linalg.norm()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca5bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
